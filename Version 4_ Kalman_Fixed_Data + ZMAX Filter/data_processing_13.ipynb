{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b3f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy.linalg import block_diag\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf17b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Synched_Data_GR0_22_DEN_MAXZ1_25/NEWDATA/'\n",
    "file_date = ['101922', '102122', '111422', '111622', '120522', '120722', \n",
    "                '013023', '020123', '031323', '031523', '041723', '041923', '061523']\n",
    "date = file_date[0]\n",
    "\n",
    "file_name = f'DAYUBIGR_{date}_GR0_22_DEN_032825_V2392628911.CSV'\n",
    "full_path = file_path + file_name\n",
    "\n",
    "raw_data = pd.read_csv(full_path, header=None, names=['SUBJECTID', 'TIME', 'X', 'Y', 'Z'])\n",
    "clear_data = raw_data.reset_index(drop=True)\n",
    "clear_data = clear_data[(clear_data[\"X\"] <= 15) & (clear_data[\"Y\"] <= 9) & \n",
    "                        (clear_data[\"X\"] >= 0) & (clear_data[\"Y\"] >= 0)].copy()\n",
    "target_subject_base = \"DS_STARFISH_2223_27\"\n",
    "subject_data = clear_data[clear_data['SUBJECTID'].str.startswith(target_subject_base)].copy()\n",
    "subject_data['TIME'] = pd.to_datetime(subject_data['TIME'])\n",
    "t0 = subject_data['TIME'].min()\n",
    "subject_data['timestamp'] = (subject_data['TIME'] - t0).dt.total_seconds()\n",
    "\n",
    "subject_data['side'] = subject_data['SUBJECTID'].str.extract(r'(\\d+[LR])$')[0].str[-1].map({'L': 'left', 'R': 'right'})\n",
    "subject_data['timestamp_rounded'] = subject_data['timestamp'].round(3)\n",
    "\n",
    "grouped = subject_data.groupby('timestamp_rounded')\n",
    "real_data = []\n",
    "\n",
    "for ts, group in grouped:\n",
    "    entry = {'timestamp': ts}\n",
    "    left = group[group['side'] == 'left']\n",
    "    right = group[group['side'] == 'right']\n",
    "    \n",
    "    if not left.empty:\n",
    "        left_xy = left[['X', 'Y']].iloc[0].to_numpy()\n",
    "        entry['left'] = left_xy\n",
    "    if not right.empty:\n",
    "        right_xy = right[['X', 'Y']].iloc[0].to_numpy()\n",
    "        entry['right'] = right_xy\n",
    "    \n",
    "    if 'left' in entry and 'right' in entry:\n",
    "        entry['observed'] = 'both'\n",
    "        entry['obs'] = np.concatenate([entry['left'], entry['right']])\n",
    "    elif 'left' in entry:\n",
    "        entry['observed'] = 'left'\n",
    "        entry['obs'] = entry['left']\n",
    "    elif 'right' in entry:\n",
    "        entry['observed'] = 'right'\n",
    "        entry['obs'] = entry['right']\n",
    "    else:\n",
    "        entry['observed'] = 'none'\n",
    "        entry['obs'] = np.array([])\n",
    "\n",
    "    real_data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ee1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_VIRT = 0.5  # Virtual time step interval\n",
    "SIGMA_MIN = 0.000001\n",
    "SIGMA_MAX = 10\n",
    "sigma_v_values = [0.01, 0.1, 0.5, 1]\n",
    "sigma_omega_values = [0.001, 0.01, 0.1, 0.5]\n",
    "sigma_obs_values = [0.5, 1]\n",
    "param_combinations = list(itertools.product(sigma_v_values, sigma_omega_values, sigma_obs_values))\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46194ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustEKF:\n",
    "    def __init__(self):\n",
    "        self.SIGMA_MIN = 1e-6\n",
    "        self.SIGMA_MAX = 10.0\n",
    "        self.DT_VIRT = 0.5\n",
    "        \n",
    "    def multiple_initialization_optimization(self, data, timestamps, virtual_timestamps, n_initializations=10):\n",
    "        best_result = None\n",
    "        best_score = np.inf\n",
    "        all_results = []\n",
    "        \n",
    "        # Define parameter bounds\n",
    "        bounds = [\n",
    "            (self.SIGMA_MIN, self.SIGMA_MAX),  # sigma_vx\n",
    "            (self.SIGMA_MIN, self.SIGMA_MAX),  # sigma_vy\n",
    "            (self.SIGMA_MIN, self.SIGMA_MAX),  # sigma_omega\n",
    "            (0.01, self.SIGMA_MAX),            # sigma_obs\n",
    "            (0.01, 1.0)                       # d\n",
    "        ]\n",
    "        \n",
    "        # Generate diverse initial guesses\n",
    "        initial_guesses = self._generate_initial_guesses(n_initializations, bounds)\n",
    "        \n",
    "        for i, init_params in enumerate(initial_guesses):\n",
    "            print(f\"Optimization attempt {i+1}/{n_initializations}\")\n",
    "            try:\n",
    "                result_lbfgs = self._optimize_single(data, timestamps, virtual_timestamps, \n",
    "                                                   init_params, bounds, method='L-BFGS-B')\n",
    "                result_nm = self._optimize_single(data, timestamps, virtual_timestamps, \n",
    "                                                init_params, bounds, method='Nelder-Mead')\n",
    "                \n",
    "                if result_lbfgs['score'] < result_nm['score']:\n",
    "                    result = result_lbfgs\n",
    "                    result['method'] = 'L-BFGS-B'\n",
    "                else:\n",
    "                    result = result_nm\n",
    "                    result['method'] = 'Nelder-Mead'\n",
    "                \n",
    "                all_results.append(result)\n",
    "                \n",
    "                if result['score'] < best_score:\n",
    "                    best_score = result['score']\n",
    "                    best_result = result\n",
    "                    \n",
    "                print(f\"  Score: {result['score']:.4f}, Method: {result['method']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Try global optimization as fallback\n",
    "        if best_result is None or len(all_results) < 3:\n",
    "            print(\"Trying global optimization (Differential Evolution)...\")\n",
    "            try:\n",
    "                global_result = self._global_optimization(data, timestamps, virtual_timestamps, bounds)\n",
    "                if global_result['score'] < best_score:\n",
    "                    best_result = global_result\n",
    "                    best_result['method'] = 'Differential Evolution'\n",
    "            except Exception as e:\n",
    "                print(f\"Global optimization failed: {str(e)}\")\n",
    "        \n",
    "        return best_result, all_results\n",
    "    \n",
    "    def _generate_initial_guesses(self, n_initializations, bounds):\n",
    "        initial_guesses = []\n",
    "        \n",
    "        # Common reasonable starting points\n",
    "        reasonable_starts = [\n",
    "            [0.1, 0.1, 0.01, 0.5, 0.23],    # Conservative\n",
    "            [0.5, 0.5, 0.1, 1.0, 0.15],     # Moderate\n",
    "            [1.0, 1.0, 0.5, 1.5, 0.3],      # Aggressive\n",
    "            [0.01, 0.01, 0.001, 0.1, 0.1],  # Very conservative\n",
    "        ]\n",
    "        \n",
    "        for start in reasonable_starts:\n",
    "            if len(initial_guesses) < n_initializations:\n",
    "                initial_guesses.append(start)\n",
    "        \n",
    "        # Random initializations for remaining slots\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        while len(initial_guesses) < n_initializations:\n",
    "            random_params = []\n",
    "            for low, high in bounds:\n",
    "                # Use log-uniform distribution for better coverage\n",
    "                if low > 0:\n",
    "                    random_params.append(np.exp(np.random.uniform(np.log(low), np.log(high))))\n",
    "                else:\n",
    "                    random_params.append(np.random.uniform(low, high))\n",
    "            initial_guesses.append(random_params)\n",
    "        \n",
    "        return initial_guesses\n",
    "    \n",
    "    def _optimize_single(self, data, timestamps, virtual_timestamps, initial_params, bounds, method):\n",
    "        def objective_with_regularization(params):\n",
    "            try:\n",
    "                _, _, _, _, neg_log_likelihood = self.ekf_forward(data, timestamps, virtual_timestamps, params)\n",
    "                \n",
    "                # Add regularization to prevent extreme parameter values\n",
    "                regularization = 0.001 * np.sum(np.log(params[:-1] + 1e-10))  # Exclude 'd' from regularization\n",
    "                \n",
    "                return neg_log_likelihood - regularization\n",
    "            except Exception as e:\n",
    "                print(f\"Objective function error: {e}\")\n",
    "                return 1e10  # Large penalty for failed evaluations\n",
    "        \n",
    "        if method == 'Nelder-Mead':\n",
    "            # Nelder-Mead doesn't support bounds directly, so we use penalties\n",
    "            def penalized_objective(params):\n",
    "                # Check bounds\n",
    "                for i, (param, (low, high)) in enumerate(zip(params, bounds)):\n",
    "                    if param < low or param > high:\n",
    "                        return 1e10  # Large penalty for out-of-bounds\n",
    "                return objective_with_regularization(params)\n",
    "            \n",
    "            result = minimize(penalized_objective, initial_params, method='Nelder-Mead',\n",
    "                            options={'maxiter': 200, 'xatol': 1e-6, 'fatol': 1e-6})\n",
    "        else:\n",
    "            result = minimize(objective_with_regularization, initial_params, method=method,\n",
    "                            bounds=bounds, options={'maxiter': 100})\n",
    "        \n",
    "        return {\n",
    "            'params': result.x,\n",
    "            'score': result.fun,\n",
    "            'success': result.success,\n",
    "            'nit': result.nit if hasattr(result, 'nit') else 0\n",
    "        }\n",
    "    \n",
    "    def _global_optimization(self, data, timestamps, virtual_timestamps, bounds):\n",
    "        \"\"\"Global optimization using Differential Evolution\"\"\"\n",
    "        def objective(params):\n",
    "            try:\n",
    "                _, _, _, _, neg_log_likelihood = self.ekf_forward(data, timestamps, virtual_timestamps, params)\n",
    "                return neg_log_likelihood\n",
    "            except:\n",
    "                return 1e10\n",
    "        \n",
    "        result = differential_evolution(objective, bounds, seed=42, maxiter=50, \n",
    "                                      popsize=10, atol=1e-6, tol=1e-6)\n",
    "        \n",
    "        return {\n",
    "            'params': result.x,\n",
    "            'score': result.fun,\n",
    "            'success': result.success,\n",
    "            'nit': result.nit\n",
    "        }\n",
    "    \n",
    "    def robust_cross_validation(self, data, timestamps, virtual_timestamps, initial_params, n_splits=3):\n",
    "        \"\"\"\n",
    "        More robust cross-validation with better error handling\n",
    "        \"\"\"\n",
    "        # Use smaller n_splits for stability\n",
    "        data_array = np.array(data)\n",
    "        timestamps_array = np.array(timestamps)\n",
    "        \n",
    "        # Ensure minimum data size per fold\n",
    "        if len(data) < n_splits * 100:  # At least 100 points per fold\n",
    "            n_splits = max(2, len(data) // 100)\n",
    "            print(f\"Reduced n_splits to {n_splits} due to limited data\")\n",
    "        \n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        fold_results = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(data_array)):\n",
    "            print(f\"Processing fold {fold + 1}/{n_splits}\")\n",
    "            \n",
    "            try:\n",
    "                train_data = data_array[train_idx].tolist()\n",
    "                val_data = data_array[val_idx].tolist()\n",
    "                train_timestamps = timestamps_array[train_idx].tolist()\n",
    "                val_timestamps = timestamps_array[val_idx].tolist()\n",
    "                \n",
    "                # Create virtual timestamps for training\n",
    "                min_time = min(train_timestamps)\n",
    "                max_time = max(train_timestamps)\n",
    "                train_virtual_timestamps = np.arange(min_time, max_time, self.DT_VIRT).tolist()\n",
    "                \n",
    "                # Optimize parameters on training fold\n",
    "                best_result, _ = self.multiple_initialization_optimization(\n",
    "                    train_data, train_timestamps, train_virtual_timestamps, n_initializations=5)\n",
    "                \n",
    "                if best_result is None:\n",
    "                    raise ValueError(\"Optimization failed for this fold\")\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                val_virtual_timestamps = np.arange(min(val_timestamps), max(val_timestamps), self.DT_VIRT).tolist()\n",
    "                _, _, _, _, val_score = self.ekf_forward(val_data, val_timestamps, \n",
    "                                                       val_virtual_timestamps, best_result['params'])\n",
    "                \n",
    "                fold_results.append({\n",
    "                    'params': best_result['params'],\n",
    "                    'val_score': val_score,\n",
    "                    'train_score': best_result['score'],\n",
    "                    'train_size': len(train_data),\n",
    "                    'val_size': len(val_data),\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "                print(f\"Fold {fold + 1} - Val score: {val_score:.2f}, Train score: {best_result['score']:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in fold {fold + 1}: {str(e)}\")\n",
    "                fold_results.append({\n",
    "                    'params': None,\n",
    "                    'val_score': np.inf,\n",
    "                    'train_score': np.inf,\n",
    "                    'train_size': len(train_data) if 'train_data' in locals() else 0,\n",
    "                    'val_size': len(val_data) if 'val_data' in locals() else 0,\n",
    "                    'success': False\n",
    "                })\n",
    "        \n",
    "        # Analyze results\n",
    "        successful_folds = [fr for fr in fold_results if fr['success']]\n",
    "        \n",
    "        if not successful_folds:\n",
    "            return {\n",
    "                'avg_val_score': np.inf,\n",
    "                'std_val_score': np.inf,\n",
    "                'best_params': None,\n",
    "                'all_fold_results': fold_results,\n",
    "                'n_successful_folds': 0\n",
    "            }\n",
    "        \n",
    "        val_scores = [fr['val_score'] for fr in successful_folds]\n",
    "        avg_val_score = np.mean(val_scores)\n",
    "        std_val_score = np.std(val_scores)\n",
    "        \n",
    "        # Select best parameters (lowest validation score)\n",
    "        best_fold = min(successful_folds, key=lambda x: x['val_score'])\n",
    "        best_params = best_fold['params']\n",
    "        \n",
    "        return {\n",
    "            'avg_val_score': avg_val_score,\n",
    "            'std_val_score': std_val_score,\n",
    "            'best_params': best_params,\n",
    "            'all_fold_results': fold_results,\n",
    "            'n_successful_folds': len(successful_folds)\n",
    "        }\n",
    "    \n",
    "    def validate_parameters(self, params):\n",
    "        sigma_vx, sigma_vy, sigma_omega, sigma_obs, d = params\n",
    "        \n",
    "        issues = []\n",
    "        \n",
    "        # Check for extreme values\n",
    "        if sigma_vx > 5.0 or sigma_vy > 5.0:\n",
    "            issues.append(\"Velocity noise parameters are very high\")\n",
    "        \n",
    "        if sigma_omega > 2.0:\n",
    "            issues.append(\"Angular velocity noise is very high\")\n",
    "        \n",
    "        if sigma_obs > 3.0:\n",
    "            issues.append(\"Observation noise is very high\")\n",
    "        \n",
    "        if d > 0.5:\n",
    "            issues.append(\"Sensor separation distance seems too large\")\n",
    "        \n",
    "        # Check for imbalanced parameters\n",
    "        velocity_ratio = max(sigma_vx, sigma_vy) / min(sigma_vx, sigma_vy)\n",
    "        if velocity_ratio > 10:\n",
    "            issues.append(\"Velocity noise parameters are highly imbalanced\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def ekf_forward(self, data, timestamps, virtual_timestamps, params):\n",
    "        \"\"\"\n",
    "        Your existing EKF forward function with improved numerical stability\n",
    "        \"\"\"\n",
    "        sigma_vx, sigma_vy, sigma_omega, sigma_obs, d = params\n",
    "        master_timestamps = sorted(set(timestamps + virtual_timestamps))\n",
    "        T = len(master_timestamps)\n",
    "        \n",
    "        # Initialize arrays\n",
    "        s_hat = [np.zeros(6)] * T\n",
    "        P = [np.zeros((6, 6))] * T\n",
    "        s_filt = [np.zeros(6)] * T\n",
    "        P_filt = [np.zeros((6, 6))] * T\n",
    "        neg_log_likelihood = 0.0\n",
    "        \n",
    "        # Improved initialization\n",
    "        s_hat[0] = np.zeros(6)\n",
    "        for entry in data[:min(10, len(data))]:\n",
    "            if entry['observed'] != 'none':\n",
    "                if entry['observed'] == 'both':\n",
    "                    s_hat[0][:2] = (entry['left'] + entry['right']) / 2\n",
    "                    break\n",
    "                elif entry['observed'] in ['left', 'right']:\n",
    "                    sensor_pos = entry['left'] if entry['observed'] == 'left' else entry['right']\n",
    "                    s_hat[0][:2] = sensor_pos\n",
    "                    break\n",
    "        \n",
    "        # More conservative initial covariance\n",
    "        P[0] = np.diag([2.0, 2.0, np.pi, 2.0, 2.0, 1.0])\n",
    "        s_filt[0] = s_hat[0].copy()\n",
    "        P_filt[0] = P[0].copy()\n",
    "        \n",
    "        for k in range(T - 1):\n",
    "            t_k = master_timestamps[k]\n",
    "            t_k1 = master_timestamps[k + 1]\n",
    "            delta_t = t_k1 - t_k\n",
    "            \n",
    "            # Prediction step\n",
    "            s_hat[k + 1] = self.state_transition(s_filt[k], delta_t)\n",
    "            F_k = self.jacobian_F(delta_t)\n",
    "            \n",
    "            # Improved process noise model\n",
    "            Q_k = np.diag([\n",
    "                0, 0, 0,  # No noise in position/angle states\n",
    "                sigma_vx**2 * delta_t,      # Velocity noise\n",
    "                sigma_vy**2 * delta_t,\n",
    "                sigma_omega**2 * delta_t    # Angular velocity noise\n",
    "            ])\n",
    "            \n",
    "            P[k + 1] = F_k @ P_filt[k] @ F_k.T + Q_k\n",
    "            \n",
    "            # Ensure covariance remains positive definite\n",
    "            P[k + 1] = (P[k + 1] + P[k + 1].T) / 2\n",
    "            eigenvals = np.linalg.eigvals(P[k + 1])\n",
    "            if np.min(eigenvals) < 1e-8:\n",
    "                P[k + 1] += np.eye(6) * 1e-6\n",
    "            \n",
    "            # Update step\n",
    "            if t_k1 in timestamps:\n",
    "                idx = timestamps.index(t_k1)\n",
    "                observed_sensors = data[idx]['observed']\n",
    "                \n",
    "                if observed_sensors != 'none':\n",
    "                    try:\n",
    "                        H_k1 = self.jacobian_h(s_hat[k + 1], observed_sensors, d)\n",
    "                        z_pred = self.h(s_hat[k + 1], observed_sensors, d)\n",
    "                        z_k1 = data[idx]['obs']\n",
    "                        m_t = len(z_k1)\n",
    "                        \n",
    "                        R = sigma_obs**2 * np.eye(m_t)\n",
    "                        S_k1 = H_k1 @ P[k + 1] @ H_k1.T + R\n",
    "                        \n",
    "                        # Ensure S_k1 is well-conditioned\n",
    "                        S_k1 = (S_k1 + S_k1.T) / 2\n",
    "                        eigenvals = np.linalg.eigvals(S_k1)\n",
    "                        if np.min(eigenvals) < 1e-10:\n",
    "                            S_k1 += np.eye(m_t) * 1e-8\n",
    "                        \n",
    "                        # Calculate innovation\n",
    "                        innovation = z_k1 - z_pred\n",
    "                        \n",
    "                        # Compute log likelihood with numerical stability\n",
    "                        try:\n",
    "                            L = np.linalg.cholesky(S_k1)\n",
    "                            log_det = 2 * np.sum(np.log(np.diag(L)))\n",
    "                            S_inv_innovation = np.linalg.solve(S_k1, innovation)\n",
    "                            \n",
    "                            neg_log_likelihood += 0.5 * (m_t * np.log(2 * np.pi) + log_det + \n",
    "                                                        innovation @ S_inv_innovation)\n",
    "                            \n",
    "                            # Kalman gain and update\n",
    "                            K_k1 = P[k + 1] @ H_k1.T @ np.linalg.solve(S_k1, np.eye(m_t))\n",
    "                            s_filt[k + 1] = s_hat[k + 1] + K_k1 @ innovation\n",
    "                            P_filt[k + 1] = (np.eye(6) - K_k1 @ H_k1) @ P[k + 1]\n",
    "                            \n",
    "                            # Ensure covariance remains positive definite\n",
    "                            P_filt[k + 1] = (P_filt[k + 1] + P_filt[k + 1].T) / 2\n",
    "                            \n",
    "                        except np.linalg.LinAlgError:\n",
    "                            # Fallback: skip this update\n",
    "                            s_filt[k + 1] = s_hat[k + 1]\n",
    "                            P_filt[k + 1] = P[k + 1]\n",
    "                            neg_log_likelihood += 1000  # Penalty for numerical issues\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        s_filt[k + 1] = s_hat[k + 1]\n",
    "                        P_filt[k + 1] = P[k + 1]\n",
    "                        neg_log_likelihood += 1000\n",
    "                else:\n",
    "                    s_filt[k + 1] = s_hat[k + 1]\n",
    "                    P_filt[k + 1] = P[k + 1]\n",
    "            else:\n",
    "                s_filt[k + 1] = s_hat[k + 1]\n",
    "                P_filt[k + 1] = P[k + 1]\n",
    "        \n",
    "        return s_filt, P_filt, s_hat, P, neg_log_likelihood\n",
    "    \n",
    "    def state_transition(self, s_t, delta_t):\n",
    "        \"\"\"State transition function\"\"\"\n",
    "        x, y, theta, vx, vy, omega = s_t\n",
    "        return np.array([\n",
    "            x + vx * delta_t,\n",
    "            y + vy * delta_t,\n",
    "            theta + omega * delta_t,\n",
    "            vx, vy, omega\n",
    "        ])\n",
    "    \n",
    "    def jacobian_F(self, delta_t):\n",
    "        \"\"\"Jacobian of state transition\"\"\"\n",
    "        return np.array([\n",
    "            [1, 0, 0, delta_t, 0, 0],\n",
    "            [0, 1, 0, 0, delta_t, 0],\n",
    "            [0, 0, 1, 0, 0, delta_t],\n",
    "            [0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 1]\n",
    "        ])\n",
    "    \n",
    "    def h(self, s_t, observed_sensors, d):\n",
    "        \"\"\"Observation function\"\"\"\n",
    "        x, y, theta = s_t[0], s_t[1], s_t[2]\n",
    "        if observed_sensors == 'both':\n",
    "            return np.array([\n",
    "                x - d * np.sin(theta), y + d * np.cos(theta),\n",
    "                x + d * np.sin(theta), y - d * np.cos(theta)\n",
    "            ])\n",
    "        elif observed_sensors == 'left':\n",
    "            return np.array([x - d * np.sin(theta), y + d * np.cos(theta)])\n",
    "        elif observed_sensors == 'right':\n",
    "            return np.array([x + d * np.sin(theta), y - d * np.cos(theta)])\n",
    "        else:\n",
    "            return np.array([])\n",
    "    \n",
    "    def jacobian_h(self, s_t, observed_sensors, d):\n",
    "        \"\"\"Jacobian of observation function\"\"\"\n",
    "        theta = s_t[2]\n",
    "        if observed_sensors == 'both':\n",
    "            return np.array([\n",
    "                [1, 0, -d * np.cos(theta), 0, 0, 0],\n",
    "                [0, 1, -d * np.sin(theta), 0, 0, 0],\n",
    "                [1, 0, d * np.cos(theta), 0, 0, 0],\n",
    "                [0, 1, d * np.sin(theta), 0, 0, 0]\n",
    "            ])\n",
    "        elif observed_sensors == 'left':\n",
    "            return np.array([\n",
    "                [1, 0, -d * np.cos(theta), 0, 0, 0],\n",
    "                [0, 1, -d * np.sin(theta), 0, 0, 0]\n",
    "            ])\n",
    "        elif observed_sensors == 'right':\n",
    "            return np.array([\n",
    "                [1, 0, d * np.cos(theta), 0, 0, 0],\n",
    "                [0, 1, d * np.sin(theta), 0, 0, 0]\n",
    "            ])\n",
    "        else:\n",
    "            return np.zeros((0, 6))\n",
    "\n",
    "# Example usage with your existing code structure\n",
    "def run_robust_optimization(real_data, param_combinations):\n",
    "    \"\"\"\n",
    "    Main function to run robust optimization with your existing parameter combinations\n",
    "    \"\"\"\n",
    "    ekf = RobustEKF()\n",
    "    results = []\n",
    "    \n",
    "    for i, (sigma_v, sigma_omega, sigma_obs) in enumerate(param_combinations):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Parameter Combination {i+1}/{len(param_combinations)}\")\n",
    "        print(f\"sigma_v: {sigma_v}, sigma_omega: {sigma_omega}, sigma_obs: {sigma_obs}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare data\n",
    "            max_data_points = min(1500, len(real_data))\n",
    "            data_subset = real_data[:max_data_points]\n",
    "            timestamps = [entry['timestamp'] for entry in data_subset]\n",
    "            virtual_timestamps = np.arange(min(timestamps), max(timestamps), ekf.DT_VIRT).tolist()\n",
    "            \n",
    "            initial_params = [sigma_v, sigma_v, sigma_omega, sigma_obs, 0.23]\n",
    "            \n",
    "            # Run robust cross-validation\n",
    "            cv_results = ekf.robust_cross_validation(data_subset, timestamps, virtual_timestamps, \n",
    "                                                   initial_params, n_splits=3)\n",
    "            \n",
    "            if cv_results['best_params'] is None:\n",
    "                raise ValueError(\"Cross-validation failed for all folds\")\n",
    "            \n",
    "            # Validate parameters\n",
    "            validation_issues = ekf.validate_parameters(cv_results['best_params'])\n",
    "            \n",
    "            # Final optimization on full dataset\n",
    "            best_result, all_optimization_results = ekf.multiple_initialization_optimization(\n",
    "                data_subset, timestamps, virtual_timestamps, n_initializations=8)\n",
    "            \n",
    "            if best_result is None:\n",
    "                raise ValueError(\"All optimization attempts failed\")\n",
    "            \n",
    "            # Create result dictionary\n",
    "            result_dict = {\n",
    "                'combination_id': i + 1,\n",
    "                'initial_sigma_v': sigma_v,\n",
    "                'initial_sigma_omega': sigma_omega,\n",
    "                'initial_sigma_obs': sigma_obs,\n",
    "                'optimized_sigma_vx': best_result['params'][0],\n",
    "                'optimized_sigma_vy': best_result['params'][1],\n",
    "                'optimized_sigma_omega': best_result['params'][2],\n",
    "                'optimized_sigma_obs': best_result['params'][3],\n",
    "                'optimized_d': best_result['params'][4],\n",
    "                'negative_log_likelihood': best_result['score'],\n",
    "                'avg_cv_score': cv_results['avg_val_score'],\n",
    "                'std_cv_score': cv_results['std_val_score'],\n",
    "                'n_successful_cv_folds': cv_results['n_successful_folds'],\n",
    "                'optimization_method': best_result['method'],\n",
    "                'n_optimization_attempts': len(all_optimization_results),\n",
    "                'validation_issues': validation_issues,\n",
    "                'optimization_success': True\n",
    "            }\n",
    "            \n",
    "            results.append(result_dict)\n",
    "            \n",
    "            print(f\"\\nRESULTS:\")\n",
    "            print(f\"Best parameters: {best_result['params']}\")\n",
    "            print(f\"Negative log-likelihood: {best_result['score']:.4f}\")\n",
    "            print(f\"CV score: {cv_results['avg_val_score']:.4f} ± {cv_results['std_cv_score']:.4f}\")\n",
    "            print(f\"Optimization method: {best_result['method']}\")\n",
    "            if validation_issues:\n",
    "                print(f\"Parameter validation issues: {validation_issues}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {str(e)}\")\n",
    "            result_dict = {\n",
    "                'combination_id': i + 1,\n",
    "                'initial_sigma_v': sigma_v,\n",
    "                'initial_sigma_omega': sigma_omega,\n",
    "                'initial_sigma_obs': sigma_obs,\n",
    "                'optimized_sigma_vx': np.nan,\n",
    "                'optimized_sigma_vy': np.nan,\n",
    "                'optimized_sigma_omega': np.nan,\n",
    "                'optimized_sigma_obs': np.nan,\n",
    "                'optimized_d': np.nan,\n",
    "                'negative_log_likelihood': np.inf,\n",
    "                'avg_cv_score': np.inf,\n",
    "                'std_cv_score': np.inf,\n",
    "                'n_successful_cv_folds': 0,\n",
    "                'optimization_method': 'failed',\n",
    "                'n_optimization_attempts': 0,\n",
    "                'validation_issues': [str(e)],\n",
    "                'optimization_success': False\n",
    "            }\n",
    "            results.append(result_dict)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb2f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Parameter Combination 1/5\n",
      "sigma_v: 0.01, sigma_omega: 0.001, sigma_obs: 0.5\n",
      "==================================================\n",
      "Processing fold 1/3\n",
      "Optimization attempt 1/5\n",
      "  Score: 1302.2625, Method: Nelder-Mead\n",
      "Optimization attempt 2/5\n",
      "  Score: 1349.4686, Method: L-BFGS-B\n",
      "Optimization attempt 3/5\n",
      "  Score: 2845.9748, Method: Nelder-Mead\n",
      "Optimization attempt 4/5\n",
      "  Score: 4743.9514, Method: L-BFGS-B\n",
      "Optimization attempt 5/5\n",
      "  Score: 2482.4291, Method: Nelder-Mead\n",
      "Fold 1 - Val score: 753.49, Train score: 1302.26\n",
      "Processing fold 2/3\n",
      "Optimization attempt 1/5\n",
      "  Score: 1266.6444, Method: L-BFGS-B\n",
      "Optimization attempt 2/5\n",
      "  Score: 1484.8390, Method: L-BFGS-B\n",
      "Optimization attempt 3/5\n",
      "  Score: 2810.0654, Method: Nelder-Mead\n",
      "Optimization attempt 4/5\n",
      "  Score: 4089.6048, Method: Nelder-Mead\n",
      "Optimization attempt 5/5\n",
      "  Score: 2448.0205, Method: Nelder-Mead\n",
      "Fold 2 - Val score: 845.30, Train score: 1266.64\n",
      "Processing fold 3/3\n",
      "Optimization attempt 1/5\n",
      "  Score: 1251.7912, Method: L-BFGS-B\n",
      "Optimization attempt 2/5\n",
      "  Score: 2183.9739, Method: Nelder-Mead\n",
      "Optimization attempt 3/5\n",
      "  Score: 2900.4323, Method: Nelder-Mead\n",
      "Optimization attempt 4/5\n",
      "  Score: 11506.3644, Method: Nelder-Mead\n",
      "Optimization attempt 5/5\n",
      "  Score: 2437.2116, Method: Nelder-Mead\n",
      "Fold 3 - Val score: 1302.22, Train score: 1251.79\n",
      "Optimization attempt 1/8\n",
      "  Score: 1729.5139, Method: Nelder-Mead\n",
      "Optimization attempt 2/8\n",
      "  Score: 1733.5135, Method: L-BFGS-B\n",
      "Optimization attempt 3/8\n",
      "  Score: 2503.4371, Method: L-BFGS-B\n",
      "Optimization attempt 4/8\n",
      "  Score: 17992.5699, Method: L-BFGS-B\n",
      "Optimization attempt 5/8\n",
      "  Score: 5008.2618, Method: Nelder-Mead\n",
      "Optimization attempt 6/8\n",
      "  Score: 5604.2931, Method: Nelder-Mead\n",
      "Optimization attempt 7/8\n",
      "  Score: 143379.4956, Method: Nelder-Mead\n",
      "Optimization attempt 8/8\n",
      "  Score: 8789.4397, Method: L-BFGS-B\n",
      "\n",
      "RESULTS:\n",
      "Best parameters: [0.03350472 0.03198535 0.01828841 0.3626606  0.43229412]\n",
      "Negative log-likelihood: 1729.5139\n",
      "ERROR: 'std_cv_score'\n",
      "\n",
      "==================================================\n",
      "Parameter Combination 2/5\n",
      "sigma_v: 0.01, sigma_omega: 0.001, sigma_obs: 1\n",
      "==================================================\n",
      "Processing fold 1/3\n",
      "Optimization attempt 1/5\n",
      "  Score: 1302.2625, Method: Nelder-Mead\n",
      "Optimization attempt 2/5\n",
      "  Score: 1349.4686, Method: L-BFGS-B\n",
      "Optimization attempt 3/5\n",
      "  Score: 2845.9748, Method: Nelder-Mead\n",
      "Optimization attempt 4/5\n",
      "  Score: 4743.9514, Method: L-BFGS-B\n",
      "Optimization attempt 5/5\n",
      "  Score: 2482.4291, Method: Nelder-Mead\n",
      "Fold 1 - Val score: 753.49, Train score: 1302.26\n",
      "Processing fold 2/3\n",
      "Optimization attempt 1/5\n",
      "  Score: 1266.6444, Method: L-BFGS-B\n",
      "Optimization attempt 2/5\n",
      "  Score: 1484.8390, Method: L-BFGS-B\n",
      "Optimization attempt 3/5\n",
      "  Score: 2810.0654, Method: Nelder-Mead\n",
      "Optimization attempt 4/5\n",
      "  Score: 4089.6048, Method: Nelder-Mead\n",
      "Optimization attempt 5/5\n",
      "  Score: 2448.0205, Method: Nelder-Mead\n",
      "Fold 2 - Val score: 845.30, Train score: 1266.64\n",
      "Processing fold 3/3\n",
      "Optimization attempt 1/5\n",
      "  Score: 1251.7912, Method: L-BFGS-B\n",
      "Optimization attempt 2/5\n",
      "  Score: 2183.9739, Method: Nelder-Mead\n",
      "Optimization attempt 3/5\n",
      "  Score: 2900.4323, Method: Nelder-Mead\n",
      "Optimization attempt 4/5\n",
      "  Score: 11506.3644, Method: Nelder-Mead\n",
      "Optimization attempt 5/5\n",
      "  Score: 2437.2116, Method: Nelder-Mead\n",
      "Fold 3 - Val score: 1302.22, Train score: 1251.79\n",
      "Optimization attempt 1/8\n",
      "  Score: 1729.5139, Method: Nelder-Mead\n",
      "Optimization attempt 2/8\n",
      "  Score: 1733.5135, Method: L-BFGS-B\n",
      "Optimization attempt 3/8\n",
      "  Score: 2503.4371, Method: L-BFGS-B\n",
      "Optimization attempt 4/8\n",
      "  Score: 17992.5699, Method: L-BFGS-B\n",
      "Optimization attempt 5/8\n"
     ]
    }
   ],
   "source": [
    "# Create the robust EKF instance\n",
    "ekf = RobustEKF()\n",
    "\n",
    "# Run with your existing parameter combinations\n",
    "results = run_robust_optimization(real_data, param_combinations[:5])  # Test with first 5 combinations\n",
    "\n",
    "# Analyze results\n",
    "best_result = min([r for r in results if r['optimization_success']], \n",
    "                  key=lambda x: x['negative_log_likelihood'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
