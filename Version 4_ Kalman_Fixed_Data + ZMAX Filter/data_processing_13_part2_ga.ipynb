{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with delta_t and DEAP algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import block_diag\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Synched_Data_GR0_22_DEN_MAXZ1_25/NEWDATA/'\n",
    "file_date = ['101922', '102122', '111422', '111622', '120522', '120722', \n",
    "                '013023', '020123', '031323', '031523', '041723', '041923', '061523']\n",
    "date = file_date[0]\n",
    "\n",
    "file_name = f'DAYUBIGR_{date}_GR0_22_DEN_032825_V2392628911.CSV'\n",
    "full_path = file_path + file_name\n",
    "\n",
    "raw_data = pd.read_csv(full_path, header=None, names=['SUBJECTID', 'TIME', 'X', 'Y', 'Z'])\n",
    "clear_data = raw_data.reset_index(drop=True)\n",
    "clear_data = clear_data[(clear_data[\"X\"] <= 15) & (clear_data[\"Y\"] <= 9) & \n",
    "                        (clear_data[\"X\"] >= 0) & (clear_data[\"Y\"] >= 0)].copy()\n",
    "target_subject_base = \"DS_STARFISH_2223_27\"\n",
    "subject_data = clear_data[clear_data['SUBJECTID'].str.startswith(target_subject_base)].copy()\n",
    "subject_data['TIME'] = pd.to_datetime(subject_data['TIME'])\n",
    "t0 = subject_data['TIME'].min()\n",
    "subject_data['timestamp'] = (subject_data['TIME'] - t0).dt.total_seconds()\n",
    "\n",
    "subject_data['side'] = subject_data['SUBJECTID'].str.extract(r'(\\d+[LR])$')[0].str[-1].map({'L': 'left', 'R': 'right'})\n",
    "subject_data['timestamp_rounded'] = subject_data['timestamp'].round(3)\n",
    "\n",
    "grouped = subject_data.groupby('timestamp_rounded')\n",
    "real_data = []\n",
    "\n",
    "for ts, group in grouped:\n",
    "    entry = {'timestamp': ts}\n",
    "    left = group[group['side'] == 'left']\n",
    "    right = group[group['side'] == 'right']\n",
    "    \n",
    "    if not left.empty:\n",
    "        left_xy = left[['X', 'Y']].iloc[0].to_numpy()\n",
    "        entry['left'] = left_xy\n",
    "    if not right.empty:\n",
    "        right_xy = right[['X', 'Y']].iloc[0].to_numpy()\n",
    "        entry['right'] = right_xy\n",
    "    \n",
    "    if 'left' in entry and 'right' in entry:\n",
    "        entry['observed'] = 'both'\n",
    "        entry['obs'] = np.concatenate([entry['left'], entry['right']])\n",
    "    elif 'left' in entry:\n",
    "        entry['observed'] = 'left'\n",
    "        entry['obs'] = entry['left']\n",
    "    elif 'right' in entry:\n",
    "        entry['observed'] = 'right'\n",
    "        entry['obs'] = entry['right']\n",
    "    else:\n",
    "        entry['observed'] = 'none'\n",
    "        entry['obs'] = np.array([])\n",
    "\n",
    "    real_data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_VIRT = 0.5  # Virtual time step interval\n",
    "SIGMA_MIN = 0.000001\n",
    "SIGMA_MAX = 10\n",
    "# sigma_v_values = [0.01, 0.05, 0.1, 0.5]\n",
    "# sigma_omega_values = [0.001, 0.01, 0.05, 0.1]\n",
    "# sigma_obs_values = [0.5, 1]\n",
    "# param_combinations = list(itertools.product(sigma_v_values, sigma_omega_values, sigma_obs_values))\n",
    "# results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_VIRT = 0.5\n",
    "SIGMA_MIN = 0.000001\n",
    "SIGMA_MAX = 10\n",
    "\n",
    "def state_transition(s_t, delta_t):\n",
    "    x, y, theta, vx, vy, omega = s_t\n",
    "    return np.array([\n",
    "        x + vx * delta_t,\n",
    "        y + vy * delta_t,\n",
    "        theta + omega * delta_t,\n",
    "        vx,\n",
    "        vy,\n",
    "        omega\n",
    "    ])\n",
    "\n",
    "def jacobian_F(delta_t):\n",
    "    return np.array([\n",
    "        [1, 0, 0, delta_t, 0, 0],\n",
    "        [0, 1, 0, 0, delta_t, 0],\n",
    "        [0, 0, 1, 0, 0, delta_t],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "def h(s_t, observed_sensors, d):\n",
    "    x, y, theta = s_t[0], s_t[1], s_t[2]\n",
    "    if observed_sensors == 'both':\n",
    "        return np.array([\n",
    "            x - d * np.sin(theta),\n",
    "            y + d * np.cos(theta),\n",
    "            x + d * np.sin(theta),\n",
    "            y - d * np.cos(theta)\n",
    "        ])\n",
    "    elif observed_sensors == 'left':\n",
    "        return np.array([x - d * np.sin(theta), y + d * np.cos(theta)])\n",
    "    elif observed_sensors == 'right':\n",
    "        return np.array([x + d * np.sin(theta), y - d * np.cos(theta)])\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "def jacobian_h(s_t, observed_sensors, d):\n",
    "    theta = s_t[2]\n",
    "    if observed_sensors == 'both':\n",
    "        return np.array([\n",
    "            [1, 0, -d * np.cos(theta), 0, 0, 0],\n",
    "            [0, 1, -d * np.sin(theta), 0, 0, 0],\n",
    "            [1, 0, d * np.cos(theta), 0, 0, 0],\n",
    "            [0, 1, d * np.sin(theta), 0, 0, 0]\n",
    "        ])\n",
    "    elif observed_sensors == 'left':\n",
    "        return np.array([\n",
    "            [1, 0, -d * np.cos(theta), 0, 0, 0],\n",
    "            [0, 1, -d * np.sin(theta), 0, 0, 0]\n",
    "        ])\n",
    "    elif observed_sensors == 'right':\n",
    "        return np.array([\n",
    "            [1, 0, d * np.cos(theta), 0, 0, 0],\n",
    "            [0, 1, d * np.sin(theta), 0, 0, 0]\n",
    "        ])\n",
    "    else:\n",
    "        return np.zeros((0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ekf_forward(data, timestamps, virtual_timestamps, params):\n",
    "    sigma_vx, sigma_vy, sigma_omega, sigma_obs, d = params\n",
    "    master_timestamps = sorted(set(timestamps + virtual_timestamps))\n",
    "    T = len(master_timestamps)\n",
    "    s_hat = [np.zeros(6)] * T\n",
    "    P = [np.zeros((6, 6))] * T\n",
    "    s_filt = [np.zeros(6)] * T\n",
    "    P_filt = [np.zeros((6, 6))] * T\n",
    "    neg_log_likelihood = 0.0\n",
    "\n",
    "    # Initialize state and covariance\n",
    "    s_hat[0] = np.zeros(6)\n",
    "    for entry in data[:10]:\n",
    "        if entry['observed'] != 'none':\n",
    "            if entry['observed'] == 'left':\n",
    "                s_hat[0][:2] = entry['left']\n",
    "                break\n",
    "            elif entry['observed'] == 'right':\n",
    "                s_hat[0][:2] = entry['right']\n",
    "                break\n",
    "            elif entry['observed'] == 'both':\n",
    "                s_hat[0][:2] = (entry['left'] + entry['right']) / 2\n",
    "                break\n",
    "                \n",
    "    P[0] = np.diag([10, 10, 10, 5, 5, 2])\n",
    "    s_filt[0] = s_hat[0]\n",
    "    P_filt[0] = P[0]\n",
    "\n",
    "    for k in range(T - 1):\n",
    "        t_k = master_timestamps[k]\n",
    "        t_k1 = master_timestamps[k + 1]\n",
    "        delta_t = t_k1 - t_k\n",
    "\n",
    "        # Prediction step\n",
    "        s_hat[k + 1] = state_transition(s_filt[k], delta_t)\n",
    "        F_k = jacobian_F(delta_t)\n",
    "        Q_k = block_diag(0, 0, 0, sigma_vx**2 * delta_t**2, sigma_vy**2 * delta_t**2, sigma_omega**2 * delta_t**2)\n",
    "        P[k + 1] = F_k @ P_filt[k] @ F_k.T + Q_k\n",
    "\n",
    "        # Update step\n",
    "        if t_k1 in timestamps:\n",
    "            idx = timestamps.index(t_k1)\n",
    "            observed_sensors = data[idx]['observed']\n",
    "            if observed_sensors != 'none':\n",
    "                H_k1 = jacobian_h(s_hat[k + 1], observed_sensors, d)\n",
    "                z_pred = h(s_hat[k + 1], observed_sensors, d)\n",
    "                z_k1 = data[idx]['obs']\n",
    "                m_t = len(z_k1)\n",
    "                R = sigma_obs**2 * np.eye(m_t)\n",
    "                S_k1 = H_k1 @ P[k + 1] @ H_k1.T + R\n",
    "                \n",
    "                S_k1 = (S_k1 + S_k1.T) / 2\n",
    "                \n",
    "                try:\n",
    "                    innovation = z_k1 - z_pred\n",
    "                    sign, logdet = np.linalg.slogdet(S_k1)\n",
    "                    if sign > 0:\n",
    "                        neg_log_likelihood += 0.5 * (m_t * np.log(2 * np.pi) + logdet + \n",
    "                                              innovation @ np.linalg.inv(S_k1) @ innovation)\n",
    "                    else:\n",
    "                        return np.inf  # Return high penalty for invalid parameters\n",
    "                    \n",
    "                    K_k1 = P[k + 1] @ H_k1.T @ np.linalg.inv(S_k1)\n",
    "                    s_filt[k + 1] = s_hat[k + 1] + K_k1 @ innovation\n",
    "                    P_filt[k + 1] = (np.eye(6) - K_k1 @ H_k1) @ P[k + 1]\n",
    "                except np.linalg.LinAlgError:\n",
    "                    return np.inf  # Return high penalty for numerical issues\n",
    "            else:\n",
    "                s_filt[k + 1] = s_hat[k + 1]\n",
    "                P_filt[k + 1] = P[k + 1]\n",
    "        else:\n",
    "            s_filt[k + 1] = s_hat[k + 1]\n",
    "            P_filt[k + 1] = P[k + 1]\n",
    "            \n",
    "    return s_filt, P_filt, s_hat, P, neg_log_likelihood\n",
    "\n",
    "def smoother(s_filt, P_filt, s_hat, P, timestamps, virtual_timestamps):\n",
    "    master_timestamps = sorted(set(timestamps + virtual_timestamps))\n",
    "    T = len(master_timestamps)\n",
    "    s_smooth = [np.zeros(6)] * T\n",
    "    P_smooth = [np.zeros((6, 6))] * T\n",
    "    s_smooth[-1] = s_filt[-1]\n",
    "    P_smooth[-1] = P_filt[-1]\n",
    "\n",
    "    for k in range(T - 2, -1, -1):\n",
    "        t_k = master_timestamps[k]\n",
    "        t_k1 = master_timestamps[k + 1]\n",
    "        delta_t = t_k1 - t_k\n",
    "        F_k = jacobian_F(delta_t)\n",
    "        \n",
    "        try:\n",
    "            C_k = P_filt[k] @ F_k.T @ np.linalg.inv(P[k + 1])\n",
    "            s_smooth[k] = s_filt[k] + C_k @ (s_smooth[k + 1] - s_hat[k + 1])\n",
    "            P_smooth[k] = P_filt[k] + C_k @ (P_smooth[k + 1] - P[k + 1]) @ C_k.T\n",
    "        except np.linalg.LinAlgError:\n",
    "            s_smooth[k] = s_filt[k]\n",
    "            P_smooth[k] = P_filt[k]\n",
    "\n",
    "    return s_smooth, P_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridOptimizer:\n",
    "    def __init__(self, data, timestamps, virtual_timestamps):\n",
    "        self.data = data\n",
    "        self.timestamps = timestamps\n",
    "        self.virtual_timestamps = virtual_timestamps\n",
    "        \n",
    "        # Parameter bounds: [sigma_vx, sigma_vy, sigma_omega, sigma_obs, d]\n",
    "        self.bounds = [\n",
    "            (SIGMA_MIN, SIGMA_MAX),\n",
    "            (SIGMA_MIN, SIGMA_MAX),\n",
    "            (SIGMA_MIN, SIGMA_MAX),\n",
    "            (0.01, SIGMA_MAX),\n",
    "            (0.01, 1.0)\n",
    "        ]\n",
    "        \n",
    "        self.setup_deap()\n",
    "        \n",
    "    def setup_deap(self):\n",
    "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))  # Not sure about weights)\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "        \n",
    "        self.toolbox = base.Toolbox()\n",
    "        \n",
    "        self.toolbox.register(\"sigma_vx\", random.uniform, self.bounds[0][0], self.bounds[0][1])\n",
    "        self.toolbox.register(\"sigma_vy\", random.uniform, self.bounds[1][0], self.bounds[1][1])\n",
    "        self.toolbox.register(\"sigma_omega\", random.uniform, self.bounds[2][0], self.bounds[2][1])\n",
    "        self.toolbox.register(\"sigma_obs\", random.uniform, self.bounds[3][0], self.bounds[3][1])\n",
    "        self.toolbox.register(\"d\", random.uniform, self.bounds[4][0], self.bounds[4][1])\n",
    "        \n",
    "        self.toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                             (self.toolbox.sigma_vx, self.toolbox.sigma_vy, \n",
    "                              self.toolbox.sigma_omega, self.toolbox.sigma_obs, self.toolbox.d), n=1)\n",
    "        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n",
    "        \n",
    "        self.toolbox.register(\"evaluate\", self.evaluate_individual)\n",
    "        self.toolbox.register(\"mate\", tools.cxBlend, alpha=0.3)\n",
    "        self.toolbox.register(\"mutate\", self.mutate_individual, mu=0, sigma=0.1, indpb=0.2)\n",
    "        self.toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "        \n",
    "    def evaluate_individual(self, individual):\n",
    "\n",
    "        params = np.array(individual)\n",
    "        for i, (param, (low, high)) in enumerate(zip(params, self.bounds)):\n",
    "            if param < low or param > high:\n",
    "                return (1e10,)  # Just in case parameters are out of bounds\n",
    "        \n",
    "        _, _, _, _, neg_log_likelihood = ekf_forward(self.data, self.timestamps, self.virtual_timestamps, params)\n",
    "        \n",
    "        if np.isnan(neg_log_likelihood) or np.isinf(neg_log_likelihood):\n",
    "            return (1e10,)\n",
    "        \n",
    "        return (neg_log_likelihood,)\n",
    "    \n",
    "    def mutate_individual(self, individual, mu, sigma, indpb):\n",
    "        for i in range(len(individual)):\n",
    "            if random.random() < indpb:\n",
    "                individual[i] += random.gauss(mu, sigma * individual[i]) # Add Gaussian noise can modify later here\n",
    "                low, high = self.bounds[i]\n",
    "                individual[i] = np.clip(individual[i], low, high)\n",
    "        return individual,\n",
    "    \n",
    "    def run_ga(self, pop_size=50, generations=30, verbose=True):\n",
    "        # Initialize population\n",
    "        pop = self.toolbox.population(n=pop_size)\n",
    "        \n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"avg\", np.mean)\n",
    "        stats.register(\"min\", np.min)\n",
    "        stats.register(\"max\", np.max)\n",
    "        \n",
    "        # Hall of fame to keep best individuals\n",
    "        hof = tools.HallOfFame(5)\n",
    "        \n",
    "        pop, logbook = algorithms.eaSimple(\n",
    "            pop, self.toolbox, cxpb=0.7, mutpb=0.3, ngen=generations,\n",
    "            stats=stats, halloffame=hof, verbose=verbose\n",
    "        )\n",
    "        \n",
    "        return hof, logbook\n",
    "    \n",
    "    def fine_tune(self, initial_params_list, method='L-BFGS-B'):\n",
    "        def objective(params):\n",
    "            try:\n",
    "                _, _, _, _, neg_log_likelihood = ekf_forward(self.data, self.timestamps, \n",
    "                                                           self.virtual_timestamps, params)\n",
    "                return neg_log_likelihood\n",
    "            except Exception:\n",
    "                return 1e10\n",
    "        \n",
    "        best_results = []\n",
    "        \n",
    "        for i, initial_params in enumerate(initial_params_list):\n",
    "            print(f\"Refining solution {i+1}/{len(initial_params_list)}\")\n",
    "            print(f\"Initial params: {initial_params}\")\n",
    "            \n",
    "            try:\n",
    "                result = minimize(\n",
    "                    objective,\n",
    "                    initial_params,\n",
    "                    method=method,\n",
    "                    bounds=self.bounds,\n",
    "                    options={'disp': False, 'maxiter': 100}\n",
    "                )\n",
    "                \n",
    "                best_results.append({\n",
    "                    'initial_params': initial_params,\n",
    "                    'optimized_params': result.x,\n",
    "                    'final_likelihood': result.fun,\n",
    "                    'success': result.success,\n",
    "                    'scipy_result': result\n",
    "                })\n",
    "                \n",
    "                print(f\"Optimized params: {result.x}\")\n",
    "                print(f\"Final likelihood: {result.fun}\")\n",
    "                print(f\"Success: {result.success}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in scipy refinement: {e}\\n\")\n",
    "                best_results.append({\n",
    "                    'initial_params': initial_params,\n",
    "                    'optimized_params': initial_params,\n",
    "                    'final_likelihood': np.inf,\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        return best_results\n",
    "    \n",
    "    def optimize(self, pop_size=50, generations=30, n_refine=3):\n",
    "        hof, logbook = self.run_ga(pop_size, generations)\n",
    "        \n",
    "        print(f\"\\n=== GA Results ===\")\n",
    "        print(\"Best individuals from genetic algorithm:\")\n",
    "        for i, ind in enumerate(hof):\n",
    "            print(f\"  {i+1}: {np.array(ind)}, fitness: {ind.fitness.values[0]}\")\n",
    "        \n",
    "        # Take top n_refine solutions for scipy refinement\n",
    "        top_solutions = [list(ind) for ind in hof[:n_refine]]\n",
    "        \n",
    "        print(f\"\\n=== Refining top {n_refine} solutions with scipy ===\")\n",
    "        refined_results = self.fine_tune(top_solutions)\n",
    "        \n",
    "        # Find best overall result\n",
    "        best_result = min(refined_results, key=lambda x: x['final_likelihood'])\n",
    "        \n",
    "        print(f\"\\n=== Best Final Result ===\")\n",
    "        print(f\"Parameters: {best_result['optimized_params']}\")\n",
    "        print(f\"Negative log-likelihood: {best_result['final_likelihood']}\")\n",
    "        print(f\"Success: {best_result['success']}\")\n",
    "        \n",
    "        return best_result, refined_results, hof, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimization_results(optimizer, best_result, title_suffix=\"\"):\n",
    "    params = best_result['optimized_params']\n",
    "\n",
    "    # Run EKF and smoother with optimized parameters\n",
    "    s_filt, P_filt, s_hat, P, nll = ekf_forward(\n",
    "        optimizer.data, optimizer.timestamps, optimizer.virtual_timestamps, params\n",
    "    )\n",
    "    s_smooth, _ = smoother(s_filt, P_filt, s_hat, P, \n",
    "                           optimizer.timestamps, optimizer.virtual_timestamps)\n",
    "\n",
    "    # Extract left/right sensor data\n",
    "    left = np.array([entry['left'] for entry in optimizer.data if 'left' in entry])\n",
    "    right = np.array([entry['right'] for entry in optimizer.data if 'right' in entry])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.title(\n",
    "        f'Hybrid Optimization {title_suffix}\\n'\n",
    "        f'sigma_vx={params[0]:.4f}, sigma_vy={params[1]:.4f}, sigma_Ï‰={params[2]:.4f}, '\n",
    "        f'sigma_obs={params[3]:.4f}, d={params[4]:.4f}\\nNLL: {nll:.2f}'\n",
    "    )\n",
    "    if left.size: plt.scatter(left[:, 0], left[:, 1], c='blue', alpha=0.3, s=10, label='Left Sensor')\n",
    "    if right.size: plt.scatter(right[:, 0], right[:, 1], c='red', alpha=0.3, s=10, label='Right Sensor')\n",
    "\n",
    "    trajectory = np.array(s_smooth)\n",
    "    plt.plot(trajectory[:, 0], trajectory[:, 1], 'g-', lw=2, alpha=0.8, label='Smoothed Trajectory')\n",
    "\n",
    "    plt.xlabel(\"X Position\")\n",
    "    plt.ylabel(\"Y Position\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return s_smooth, _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg         \tmin    \tmax         \n",
      "0  \t40    \t1.73541e+180\t13078.9\t6.94164e+181\n",
      "1  \t36    \t1.00002e+09 \t11712.9\t1e+10       \n",
      "2  \t29    \t7.50014e+08 \t11712.9\t1e+10       \n",
      "3  \t32    \t1.00001e+09 \t10407.6\t1e+10       \n",
      "4  \t30    \t12443.9     \t10407.6\t14909.4     \n",
      "5  \t35    \t11573.3     \t10052.1\t12927.6     \n"
     ]
    }
   ],
   "source": [
    "def run_hybrid_optimization_with_your_data():\n",
    "    max_data_points = 3000\n",
    "    data_subset = real_data[:max_data_points]\n",
    "    timestamps = [entry['timestamp'] for entry in data_subset]\n",
    "    virtual_timestamps = np.arange(min(timestamps), max(timestamps), DT_VIRT).tolist()\n",
    "\n",
    "    optimizer = HybridOptimizer(data_subset, timestamps, virtual_timestamps)\n",
    "\n",
    "    best_result, refined_results, hof, logbook = optimizer.optimize(\n",
    "        pop_size=40,\n",
    "        generations=25,\n",
    "        n_refine=3        # refine top 3 solutions with SciPy\n",
    "    )\n",
    "\n",
    "    s_smooth, P_smooth = visualize_optimization_results(optimizer, best_result, title_suffix=f\"(Date: {date})\")\n",
    "\n",
    "    results_data = []\n",
    "\n",
    "    # GA results\n",
    "    for i, ind in enumerate(hof):\n",
    "        results_data.append({\n",
    "            'method': 'genetic_algorithm',\n",
    "            'rank': i + 1,\n",
    "            'sigma_vx': ind[0],\n",
    "            'sigma_vy': ind[1],\n",
    "            'sigma_omega': ind[2],\n",
    "            'sigma_obs': ind[3],\n",
    "            'd': ind[4],\n",
    "            'negative_log_likelihood': ind.fitness.values[0],\n",
    "            'stage': 'GA_only'\n",
    "        })\n",
    "\n",
    "    # GA + SciPy refined results\n",
    "    for i, result in enumerate(refined_results):\n",
    "        results_data.append({\n",
    "            'method': 'hybrid_ga_scipy',\n",
    "            'rank': i + 1,\n",
    "            'sigma_vx': result['optimized_params'][0],\n",
    "            'sigma_vy': result['optimized_params'][1],\n",
    "            'sigma_omega': result['optimized_params'][2],\n",
    "            'sigma_obs': result['optimized_params'][3],\n",
    "            'd': result['optimized_params'][4],\n",
    "            'negative_log_likelihood': result['final_likelihood'],\n",
    "            'stage': 'GA_then_scipy',\n",
    "            'scipy_success': result['success']\n",
    "        })\n",
    "\n",
    "    # results_df = pd.DataFrame(results_data)\n",
    "    return best_result, refined_results, hof, logbook, optimizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_result, refined_results, hof, logbook, optimizer = run_hybrid_optimization_with_your_data()\n",
    "    \n",
    "    visualize_optimization_results(optimizer, best_result, title_suffix=\"(Best Result)\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
